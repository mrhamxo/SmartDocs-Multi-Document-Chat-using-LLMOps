{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71618c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543ffe0",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc675fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"Question: what is R language and where we used it?\",\n",
    "    \"define and explain predictive model?\",\n",
    "    \"what is unsupervised learning?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"R is a free, open‑source programming language designed for statistical computing and graphics. It is the preferred tool for academic statisticians and data scientists, enabling the implementation of a wide range of statistical methods and the creation of plots, models, and simulations. R is used for data analysis, model building, and visualizing results in research, industry, and education.\",\n",
    "    \"A predictive model is a statistical algorithm that uses one or more predictor variables to estimate or forecast the value of a response variable. It learns the relationship between predictors and the response from training data—often via techniques like linear regression—and then applies that learned relationship to new data to make predictions. The model’s usefulness is judged by how well it explains the response and how accurately it predicts unseen observations.\",\n",
    "    \" Unsupervised learning is a type of machine learning where the data have no labeled outputs, so the algorithm must discover patterns, structure, or groupings on its own—such as clustering or dimensionality‑reduction techniques—without a known “true answer” to validate against.\",\n",
    "]\n",
    "\n",
    "# Dataset\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Write to csv\n",
    "csv_path = \"G:/Data Science/MLOPs/SmartDocs-Multi-Document-Chat-using-LLMOps/data/Q&A.csv\"\n",
    "\n",
    "df.to_csv(csv_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c95c51",
   "metadata": {},
   "source": [
    "### Upload Dataset on Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3866f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"llmops_dataset\"\n",
    "description=\"Input and expected output pairs for R Language PDF document.\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name = dataset_name,\n",
    "    description = description\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs = [{\"question\": q} for q in inputs],\n",
    "    outputs = [{\"answer\": a} for a in outputs],\n",
    "    dataset_id = dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12ab01",
   "metadata": {},
   "source": [
    "### Upload RAG Application on Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f86be517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"G:/Data Science/MLOPs/SmartDocs-Multi-Document-Chat-using-LLMOps\")\n",
    "from pathlib import Path\n",
    "from multi_doc_chat.src.document_ingestion.data_ingestion import ChatIngestor\n",
    "from multi_doc_chat.src.document_chat.retrieval import ConversationalRAG\n",
    "import os\n",
    "\n",
    "\n",
    "# Simple file adapter for local file paths\n",
    "class LocalFileAdapter:\n",
    "    \"\"\"Adapter for local file paths to work with ChatIngestor.\"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.path = Path(file_path)\n",
    "        self.name = self.path.name\n",
    "    \n",
    "    def getbuffer(self) -> bytes:\n",
    "        return self.path.read_bytes()\n",
    "\n",
    "\n",
    "def answer_ai_report_question(\n",
    "    inputs: dict,\n",
    "    data_path: str = \"G:/Data Science/MLOPs/SmartDocs-Multi-Document-Chat-using-LLMOps\\data\\islr.pdf\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Answer questions about the AI Engineering Report using RAG.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing the question, e.g., {\"question\": \"What is RAG?\"}\n",
    "        data_path: Path to the AI Engineering Report text file\n",
    "        chunk_size: Size of text chunks for splitting\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with the answer, e.g., {\"answer\": \"RAG stands for...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract question from inputs\n",
    "        question = inputs.get(\"question\", \"\")\n",
    "        if not question:\n",
    "            return {\"answer\": \"No question provided\"}\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not Path(data_path).exists():\n",
    "            return {\"answer\": f\"Data file not found: {data_path}\"}\n",
    "        \n",
    "        # Create file adapter\n",
    "        file_adapter = LocalFileAdapter(data_path)\n",
    "        \n",
    "        # Build index using ChatIngestor\n",
    "        ingestor = ChatIngestor(\n",
    "            temp_base=\"data\",\n",
    "            faiss_base=\"faiss_index\",\n",
    "            use_session_dirs=True\n",
    "        )\n",
    "        \n",
    "        # Build retriever\n",
    "        ingestor.built_retriver(\n",
    "            uploaded_files=[file_adapter],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        # Get session ID and index path\n",
    "        session_id = ingestor.session_id\n",
    "        index_path = f\"faiss_index/{session_id}\"\n",
    "        \n",
    "        # Create RAG instance and load retriever\n",
    "        rag = ConversationalRAG(session_id=session_id)\n",
    "        rag.load_retriever_from_faiss(\n",
    "            index_path=index_path,\n",
    "            k=k,\n",
    "            index_name=os.getenv(\"FAISS_INDEX_NAME\", \"index\")\n",
    "        )\n",
    "        \n",
    "        # Get answer\n",
    "        answer = rag.invoke(question, chat_history=[])\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9531f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-10-24T20:38:26.452016Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:38:26.454020Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:38:26.456017Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:38:26.459016Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:38:26.520924Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251024_163826_4028e278\", \"temp_dir\": \"data\\\\session_20251024_163826_4028e278\", \"faiss_dir\": \"faiss_index\\\\session_20251024_163826_4028e278\", \"sessionized\": true, \"timestamp\": \"2025-10-24T20:38:26.523928Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"islr.pdf\", \"saved_as\": \"data\\\\session_20251024_163826_4028e278\\\\4ed94952.pdf\", \"timestamp\": \"2025-10-24T20:38:26.780965Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 441, \"timestamp\": \"2025-10-24T20:39:46.522935Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 1307, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-24T20:39:46.634311Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T20:39:46.636294Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251024_163826_4028e278\", \"timestamp\": \"2025-10-24T20:40:08.072241Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-24T20:40:08.076239Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-24T20:40:08.087835Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:40:08.092909Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:40:08.095839Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:40:08.097836Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:40:08.099151Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2025-10-24T20:40:08.099151Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251024_163826_4028e278\", \"timestamp\": \"2025-10-24T20:40:08.843734Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251024_163826_4028e278\", \"timestamp\": \"2025-10-24T20:40:08.843734Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-24T20:40:08.843734Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:40:08.859361Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:40:08.860679Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:40:08.860679Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:40:08.860679Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T20:40:08.860679Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251024_163826_4028e278\", \"timestamp\": \"2025-10-24T20:40:09.632373Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251024_163826_4028e278\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251024_163826_4028e278\", \"timestamp\": \"2025-10-24T20:40:09.632373Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251024_163826_4028e278\", \"user_input\": \"what is R language and where we used it?\", \"answer_preview\": \"R is a free, open\\u2011source programming language designed for statistical computing and graphics. It is the preferred tool for academic statisticians and\", \"timestamp\": \"2025-10-24T20:40:10.861566Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is R language and where we used it?\n",
      "\n",
      "Answer: R is a free, open‑source programming language designed for statistical computing and graphics. It is the preferred tool for academic statisticians and data scientists, enabling the implementation of a wide range of statistical methods and the creation of plots, models, and simulations. R is used for data analysis, model building, and visualizing results in research, industry, and education.\n"
     ]
    }
   ],
   "source": [
    "# Test the function with a sample question\n",
    "test_input = {\"question\": \"what is R language and where we used it?\"}\n",
    "result = answer_ai_report_question(test_input)\n",
    "\n",
    "print(\"Question:\", test_input[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d049e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all questions from the dataset:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-10-24T20:40:20.679665Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:40:20.682666Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:40:20.684665Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:40:20.686665Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:40:20.692666Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251024_164020_de9d1067\", \"temp_dir\": \"data\\\\session_20251024_164020_de9d1067\", \"faiss_dir\": \"faiss_index\\\\session_20251024_164020_de9d1067\", \"sessionized\": true, \"timestamp\": \"2025-10-24T20:40:20.725550Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"islr.pdf\", \"saved_as\": \"data\\\\session_20251024_164020_de9d1067\\\\6b14222d.pdf\", \"timestamp\": \"2025-10-24T20:40:20.778204Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 441, \"timestamp\": \"2025-10-24T20:41:48.984600Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 1307, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-24T20:41:49.111681Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T20:41:49.114681Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251024_164020_de9d1067\", \"timestamp\": \"2025-10-24T20:42:08.382827Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-24T20:42:08.385832Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-24T20:42:08.394215Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:42:08.397703Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:42:08.399701Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:42:08.401701Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:42:08.409004Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2025-10-24T20:42:08.411002Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251024_164020_de9d1067\", \"timestamp\": \"2025-10-24T20:42:09.334987Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251024_164020_de9d1067\", \"timestamp\": \"2025-10-24T20:42:09.336991Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-24T20:42:09.340988Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:42:09.342987Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:42:09.343988Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:42:09.344987Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:42:09.348987Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T20:42:09.350990Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251024_164020_de9d1067\", \"timestamp\": \"2025-10-24T20:42:09.443373Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251024_164020_de9d1067\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251024_164020_de9d1067\", \"timestamp\": \"2025-10-24T20:42:09.445373Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251024_164020_de9d1067\", \"user_input\": \"Question: what is R language and where we used it?\", \"answer_preview\": \"R is a freely available programming language designed for statistical computing and data analysis. It is the language of choice for academic statistic\", \"timestamp\": \"2025-10-24T20:42:10.312813Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Question: what is R language and where we used it?\n",
      "A1: R is a freely available programming language designed for statistical computing and data analysis. It is the language of choice for academic statisticians and is used to implement a wide range of statistical methods, often with optional packages that add thousands of additional functions. R is employed in research, teaching, and any context where advanced data analysis or modeling is required.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-10-24T20:42:10.327792Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:42:10.330794Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:42:10.331789Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:42:10.333791Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:42:10.340791Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251024_164210_682d339a\", \"temp_dir\": \"data\\\\session_20251024_164210_682d339a\", \"faiss_dir\": \"faiss_index\\\\session_20251024_164210_682d339a\", \"sessionized\": true, \"timestamp\": \"2025-10-24T20:42:10.343792Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"islr.pdf\", \"saved_as\": \"data\\\\session_20251024_164210_682d339a\\\\153246ed.pdf\", \"timestamp\": \"2025-10-24T20:42:10.404618Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 441, \"timestamp\": \"2025-10-24T20:43:40.300087Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 1307, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-24T20:43:40.452473Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T20:43:40.452473Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251024_164210_682d339a\", \"timestamp\": \"2025-10-24T20:44:00.654295Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-24T20:44:00.655360Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-24T20:44:00.655360Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:44:00.655360Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:44:00.666116Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:44:00.668198Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:44:00.679274Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2025-10-24T20:44:00.685727Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251024_164210_682d339a\", \"timestamp\": \"2025-10-24T20:44:01.560878Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251024_164210_682d339a\", \"timestamp\": \"2025-10-24T20:44:01.560878Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-24T20:44:01.560878Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:44:01.560878Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:44:01.560878Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:44:01.576505Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:44:01.576505Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T20:44:01.576505Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251024_164210_682d339a\", \"timestamp\": \"2025-10-24T20:44:01.675057Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251024_164210_682d339a\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251024_164210_682d339a\", \"timestamp\": \"2025-10-24T20:44:01.677055Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251024_164210_682d339a\", \"user_input\": \"define and explain predictive model?\", \"answer_preview\": \"A predictive model is a statistical or machine\\u2011learning framework that learns a relationship between input variables (predictors) and an outcome (resp\", \"timestamp\": \"2025-10-24T20:44:02.600742Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: define and explain predictive model?\n",
      "A2: A predictive model is a statistical or machine‑learning framework that learns a relationship between input variables (predictors) and an outcome (response) from training data, and then uses that learned relationship to forecast future or unseen outcomes. In practice, the model is fitted to a training set, its parameters are estimated, and its performance is evaluated on a separate test set to gauge accuracy. Simple approaches such as linear or logistic regression are common examples, but more complex models can be used when the underlying relationship is not adequately captured by a linear form.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2025-10-24T20:44:02.614926Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:44:02.617927Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:44:02.619928Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:44:02.623681Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:44:02.633129Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251024_164402_00052534\", \"temp_dir\": \"data\\\\session_20251024_164402_00052534\", \"faiss_dir\": \"faiss_index\\\\session_20251024_164402_00052534\", \"sessionized\": true, \"timestamp\": \"2025-10-24T20:44:02.637132Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"islr.pdf\", \"saved_as\": \"data\\\\session_20251024_164402_00052534\\\\c4d08c0a.pdf\", \"timestamp\": \"2025-10-24T20:44:02.657954Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 441, \"timestamp\": \"2025-10-24T20:45:25.561331Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 1307, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-24T20:45:25.646267Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T20:45:25.646267Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251024_164402_00052534\", \"timestamp\": \"2025-10-24T20:45:44.530789Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-24T20:45:44.530789Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-24T20:45:44.530789Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:45:44.546771Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:45:44.548347Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:45:44.550350Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:45:44.557348Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2025-10-24T20:45:44.559348Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251024_164402_00052534\", \"timestamp\": \"2025-10-24T20:45:45.629465Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251024_164402_00052534\", \"timestamp\": \"2025-10-24T20:45:45.632467Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-24T20:45:45.636467Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T20:45:45.639465Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T20:45:45.640884Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T20:45:45.642883Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T20:45:45.652885Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T20:45:45.655937Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251024_164402_00052534\", \"timestamp\": \"2025-10-24T20:45:45.752929Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251024_164402_00052534\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251024_164402_00052534\", \"timestamp\": \"2025-10-24T20:45:45.752929Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251024_164402_00052534\", \"user_input\": \"what is unsupervised learning?\", \"answer_preview\": \"Unsupervised learning analyzes data that has no labeled outputs, aiming to discover hidden structure or patterns such as clusters or low\\u2011dimensional r\", \"timestamp\": \"2025-10-24T20:45:46.639060Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3: what is unsupervised learning?\n",
      "A3: Unsupervised learning analyzes data that has no labeled outputs, aiming to discover hidden structure or patterns such as clusters or low‑dimensional representations. It is often used in exploratory data analysis, where there is no clear prediction target. Typical methods include clustering, principal component analysis, and other dimensionality‑reduction techniques.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Test with all questions and Answers from the dataset\n",
    "print(\"Testing all questions from the dataset:\\n\")\n",
    "for i, q in enumerate(inputs, 1):\n",
    "    test_input = {\"question\": q}\n",
    "    result = answer_ai_report_question(test_input)\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    print(f\"A{i}: {result['answer']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcdc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Evaluators\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"llmops_dataset\"\n",
    "\n",
    "# Run evaluation using our RAG function\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-agenticAIReport-qa-rag\",\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and R language PDF\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7d74b",
   "metadata": {},
   "source": [
    "### Custom Correctness Evaluator\n",
    "    Creating an LLM-as-a-Judge evaluator to assess semantic and factual alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b5efb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def correctness_evaluator(run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Custom LLM-as-a-Judge evaluator for correctness.\n",
    "    \n",
    "    Correctness means how well the actual model output matches the reference output \n",
    "    in terms of factual accuracy, coverage, and meaning.\n",
    "    \n",
    "    Args:\n",
    "        run: The Run object containing the actual outputs\n",
    "        example: The Example object containing the expected outputs\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'score' (1 for correct, 0 for incorrect) and 'reasoning'\n",
    "    \"\"\"\n",
    "    # Extract actual and expected outputs\n",
    "    actual_output = run.outputs.get(\"answer\", \"\")\n",
    "    expected_output = example.outputs.get(\"answer\", \"\")\n",
    "    input_question = example.inputs.get(\"question\", \"\")\n",
    "    \n",
    "    # Define the evaluation prompt\n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an evaluator whose job is to judge correctness.\n",
    "\n",
    "Correctness means how well the actual model output matches the reference output in terms of factual accuracy, coverage, and meaning.\n",
    "\n",
    "- If the actual output matches the reference output semantically (even if wording differs), it should be marked correct.\n",
    "- If the output misses key facts, introduces contradictions, or is factually incorrect, it should be marked incorrect.\n",
    "\n",
    "Do not penalize for stylistic or formatting differences unless they change meaning.\"\"\"),\n",
    "        (\"human\", \"\"\"<example>\n",
    "<input>\n",
    "{input}\n",
    "</input>\n",
    "\n",
    "<output>\n",
    "Expected Output: {expected_output}\n",
    "\n",
    "Actual Output: {actual_output}\n",
    "</output>\n",
    "</example>\n",
    "\n",
    "Please grade the following agent run given the input, expected output, and actual output.\n",
    "Focus only on correctness (semantic and factual alignment).\n",
    "\n",
    "Respond with:\n",
    "1. A brief reasoning (1-2 sentences)\n",
    "2. A final verdict: either \"CORRECT\" or \"INCORRECT\"\n",
    "\n",
    "Format your response as:\n",
    "Reasoning: [your reasoning]\n",
    "Verdict: [CORRECT or INCORRECT]\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Initialize LLM (using Gemini as shown in your config)\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Create chain and invoke\n",
    "    chain = eval_prompt | llm\n",
    "    \n",
    "    try:\n",
    "        response = chain.invoke({\n",
    "            \"input\": input_question,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"actual_output\": actual_output\n",
    "        })\n",
    "        \n",
    "        response_text = response.content\n",
    "        \n",
    "        # Parse the response\n",
    "        reasoning = \"\"\n",
    "        verdict = \"\"\n",
    "        \n",
    "        for line in response_text.split('\\n'):\n",
    "            if line.startswith(\"Reasoning:\"):\n",
    "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
    "            elif line.startswith(\"Verdict:\"):\n",
    "                verdict = line.replace(\"Verdict:\", \"\").strip()\n",
    "        \n",
    "        # Convert verdict to score (1 for correct, 0 for incorrect)\n",
    "        score = 1 if \"CORRECT\" in verdict.upper() else 0\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": score,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"comment\": f\"Verdict: {verdict}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"correctness\",\n",
    "            \"score\": 0,\n",
    "            \"reasoning\": f\"Error during evaluation: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1dfd4",
   "metadata": {},
   "source": [
    "### Run Evaluation with Custom Correctness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "815ce4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'agenticAIReport-correctness-eval-df2f0942' at:\n",
      "https://smith.langchain.com/o/82d84e60-5c73-4bf6-a898-7a09feddd805/datasets/f43b02e9-ab76-4e87-89f9-d512be08d9cb/compare?selectedSessions=0983e2e4-b58c-4297-8419-d2a6e350baaa\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Data Science\\MLOPs\\SmartDocs-Multi-Document-Chat-using-LLMOps\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "0it [00:00, ?it/s]{\"timestamp\": \"2025-10-24T21:18:31.944868Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T21:18:31.944868Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T21:18:31.944868Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T21:18:31.944868Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T21:18:31.979199Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251024_171831_8e51b681\", \"temp_dir\": \"data\\\\session_20251024_171831_8e51b681\", \"faiss_dir\": \"faiss_index\\\\session_20251024_171831_8e51b681\", \"sessionized\": true, \"timestamp\": \"2025-10-24T21:18:31.979199Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"islr.pdf\", \"saved_as\": \"data\\\\session_20251024_171831_8e51b681\\\\a0211e5d.pdf\", \"timestamp\": \"2025-10-24T21:18:32.227758Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 441, \"timestamp\": \"2025-10-24T21:19:51.022440Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 1307, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-24T21:19:51.116270Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T21:19:51.118271Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251024_171831_8e51b681\", \"timestamp\": \"2025-10-24T21:20:10.476688Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-24T21:20:10.479692Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-24T21:20:10.490470Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T21:20:10.492473Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T21:20:10.496478Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T21:20:10.499471Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T21:20:10.510469Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2025-10-24T21:20:10.513475Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251024_171831_8e51b681\", \"timestamp\": \"2025-10-24T21:20:11.272752Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251024_171831_8e51b681\", \"timestamp\": \"2025-10-24T21:20:11.274756Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-24T21:20:11.277749Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T21:20:11.279766Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T21:20:11.281748Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T21:20:11.283748Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T21:20:11.290748Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T21:20:11.293752Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251024_171831_8e51b681\", \"timestamp\": \"2025-10-24T21:20:11.376751Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251024_171831_8e51b681\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251024_171831_8e51b681\", \"timestamp\": \"2025-10-24T21:20:11.378753Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251024_171831_8e51b681\", \"user_input\": \"How often are most respondents updating their models?\", \"answer_preview\": \"I don't know.\", \"timestamp\": \"2025-10-24T21:20:12.296670Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "1it [01:45, 105.44s/it]{\"timestamp\": \"2025-10-24T21:20:17.389597Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T21:20:17.391599Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T21:20:17.393598Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T21:20:17.395598Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T21:20:17.402599Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251024_172017_aa97baab\", \"temp_dir\": \"data\\\\session_20251024_172017_aa97baab\", \"faiss_dir\": \"faiss_index\\\\session_20251024_172017_aa97baab\", \"sessionized\": true, \"timestamp\": \"2025-10-24T21:20:17.414596Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"islr.pdf\", \"saved_as\": \"data\\\\session_20251024_172017_aa97baab\\\\aa7f1367.pdf\", \"timestamp\": \"2025-10-24T21:20:17.432600Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 441, \"timestamp\": \"2025-10-24T21:21:29.624316Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 1307, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-24T21:21:29.703338Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T21:21:29.705321Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251024_172017_aa97baab\", \"timestamp\": \"2025-10-24T21:21:49.700583Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-24T21:21:49.709573Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-24T21:21:49.717593Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T21:21:49.720573Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T21:21:49.723580Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T21:21:49.726571Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T21:21:49.735572Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2025-10-24T21:21:49.739572Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251024_172017_aa97baab\", \"timestamp\": \"2025-10-24T21:21:50.509807Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251024_172017_aa97baab\", \"timestamp\": \"2025-10-24T21:21:50.510785Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-24T21:21:50.513786Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T21:21:50.515786Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T21:21:50.516785Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T21:21:50.518786Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T21:21:50.525786Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T21:21:50.527788Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251024_172017_aa97baab\", \"timestamp\": \"2025-10-24T21:21:50.579789Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251024_172017_aa97baab\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251024_172017_aa97baab\", \"timestamp\": \"2025-10-24T21:21:50.581791Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251024_172017_aa97baab\", \"user_input\": \"For customer-facing applications, which company's models dominate the top rankings?\", \"answer_preview\": \"I don't know.\", \"timestamp\": \"2025-10-24T21:21:51.707945Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "2it [03:25, 102.12s/it]{\"timestamp\": \"2025-10-24T21:21:57.193652Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T21:21:57.196663Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T21:21:57.199663Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T21:21:57.204651Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T21:21:57.214649Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"session_id\": \"session_20251024_172157_1e54a229\", \"temp_dir\": \"data\\\\session_20251024_172157_1e54a229\", \"faiss_dir\": \"faiss_index\\\\session_20251024_172157_1e54a229\", \"sessionized\": true, \"timestamp\": \"2025-10-24T21:21:57.219650Z\", \"level\": \"info\", \"event\": \"ChatIngestor initialized\"}\n",
      "{\"uploaded\": \"islr.pdf\", \"saved_as\": \"data\\\\session_20251024_172157_1e54a229\\\\3ffc3bf1.pdf\", \"timestamp\": \"2025-10-24T21:21:57.235650Z\", \"level\": \"info\", \"event\": \"File saved for ingestion\"}\n",
      "{\"count\": 441, \"timestamp\": \"2025-10-24T21:23:07.756162Z\", \"level\": \"info\", \"event\": \"Documents loaded\"}\n",
      "{\"chunks\": 1307, \"chunk_size\": 1000, \"overlap\": 200, \"timestamp\": \"2025-10-24T21:23:07.834186Z\", \"level\": \"info\", \"event\": \"Documents split\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T21:23:07.836165Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"added\": 1, \"index\": \"faiss_index\\\\session_20251024_172157_1e54a229\", \"timestamp\": \"2025-10-24T21:23:27.291157Z\", \"level\": \"info\", \"event\": \"FAISS index updated\"}\n",
      "{\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"timestamp\": \"2025-10-24T21:23:27.296159Z\", \"level\": \"info\", \"event\": \"Using MMR search\"}\n",
      "{\"timestamp\": \"2025-10-24T21:23:27.305335Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T21:23:27.306324Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T21:23:27.308315Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T21:23:27.310315Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T21:23:27.317318Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"provider\": \"groq\", \"model\": \"openai/gpt-oss-20b\", \"timestamp\": \"2025-10-24T21:23:27.320314Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n",
      "{\"session_id\": \"session_20251024_172157_1e54a229\", \"timestamp\": \"2025-10-24T21:23:28.101908Z\", \"level\": \"info\", \"event\": \"LLM loaded successfully\"}\n",
      "{\"session_id\": \"session_20251024_172157_1e54a229\", \"timestamp\": \"2025-10-24T21:23:28.103902Z\", \"level\": \"info\", \"event\": \"ConversationalRAG initialized\"}\n",
      "{\"timestamp\": \"2025-10-24T21:23:28.106903Z\", \"level\": \"info\", \"event\": \"Running in LOCAL mode: .env loaded\"}\n",
      "{\"timestamp\": \"2025-10-24T21:23:28.108903Z\", \"level\": \"info\", \"event\": \"Loaded GROQ_API_KEY from individual env var\"}\n",
      "{\"timestamp\": \"2025-10-24T21:23:28.110909Z\", \"level\": \"info\", \"event\": \"Loaded GOOGLE_API_KEY from individual env var\"}\n",
      "{\"keys\": {\"GROQ_API_KEY\": \"gsk_6Z...\", \"GOOGLE_API_KEY\": \"AIzaSy...\"}, \"timestamp\": \"2025-10-24T21:23:28.111903Z\", \"level\": \"info\", \"event\": \"API keys loaded\"}\n",
      "{\"config_keys\": [\"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2025-10-24T21:23:28.115959Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n",
      "{\"model\": \"models/text-embedding-004\", \"timestamp\": \"2025-10-24T21:23:28.118969Z\", \"level\": \"info\", \"event\": \"Loading embedding model\"}\n",
      "{\"session_id\": \"session_20251024_172157_1e54a229\", \"timestamp\": \"2025-10-24T21:23:28.180982Z\", \"level\": \"info\", \"event\": \"LCEL graph built successfully\"}\n",
      "{\"index_path\": \"faiss_index/session_20251024_172157_1e54a229\", \"index_name\": \"index\", \"search_type\": \"mmr\", \"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5, \"session_id\": \"session_20251024_172157_1e54a229\", \"timestamp\": \"2025-10-24T21:23:28.182961Z\", \"level\": \"info\", \"event\": \"FAISS retriever loaded successfully\"}\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "{\"session_id\": \"session_20251024_172157_1e54a229\", \"user_input\": \"What percentage of respondents are using RAG in some form?\", \"answer_preview\": \"I don't know.\", \"timestamp\": \"2025-10-24T21:23:28.913310Z\", \"level\": \"info\", \"event\": \"Chain invoked successfully\"}\n",
      "3it [05:02, 100.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed! Check the LangSmith UI for detailed results.\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with the custom correctness evaluator\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define evaluators - using custom correctness evaluator\n",
    "evaluators = [correctness_evaluator]\n",
    "\n",
    "dataset_name = \"llmops_dataset\"\n",
    "\n",
    "# Run evaluation\n",
    "experiment_results = evaluate(\n",
    "    answer_ai_report_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=\"agenticAIReport-correctness-eval\",\n",
    "    description=\"Evaluating RAG system with custom correctness evaluator (LLM-as-a-Judge)\",\n",
    "    metadata={\n",
    "        \"variant\": \"RAG with FAISS and AI Engineering Report\",\n",
    "        \"evaluator\": \"custom_correctness_llm_judge\",\n",
    "        \"model\": \"gemini-2.5-pro\",\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"k\": 5,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation completed! Check the LangSmith UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f868ec6",
   "metadata": {},
   "source": [
    "### Optional: Combine Multiple Evaluators\n",
    "    You can use multiple evaluators together to get different perspectives on your RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Combine custom correctness evaluator with LangChain's built-in evaluators\n",
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Combine custom and built-in evaluators\n",
    "combined_evaluators = [\n",
    "    correctness_evaluator,  # Custom LLM-as-a-Judge\n",
    "    LangChainStringEvaluator(\"cot_qa\"),  # Chain-of-thought QA evaluator\n",
    "]\n",
    "\n",
    "# Run evaluation with multiple evaluators\n",
    "# Uncomment to run:\n",
    "# experiment_results_combined = evaluate(\n",
    "#     answer_ai_report_question,\n",
    "#     data=dataset_name,\n",
    "#     evaluators=combined_evaluators,\n",
    "#     experiment_prefix=\"agenticAIReport-multi-eval\",\n",
    "#     description=\"Evaluating RAG system with multiple evaluators\",\n",
    "#     metadata={\n",
    "#         \"variant\": \"RAG with FAISS\",\n",
    "#         \"evaluators\": \"correctness + cot_qa\",\n",
    "#         \"chunk_size\": 1000,\n",
    "#         \"chunk_overlap\": 200,\n",
    "#         \"k\": 5,\n",
    "#     },\n",
    "# )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa47aa19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartdocs-multi-document-chat-using-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

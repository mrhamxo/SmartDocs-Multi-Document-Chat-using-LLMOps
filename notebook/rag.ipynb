{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adad0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.9 environment at: G:\\Data Science\\MLOPs\\LLMOPS series\\.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m8 packages\u001b[0m \u001b[2min 8.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install langchain groq tiktoken rapidocr-onnxruntime python-dotenv langchain-community sentence-transformers faiss-cpu langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7026b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if groq_api_key is None:\n",
    "\traise ValueError(\"GROQ_API_KEY not found in environment variables. Please check your .env file.\")\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = groq_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f56165",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/sample_text.txt\", encoding=\"utf8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a6b6c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neural Radiance Fields (NeRF) represent a cutting-edge technique in 3D computer graphics that uses n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content[ : 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d3da8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/sample_text.txt'}, page_content='Neural Radiance Fields (NeRF) represent a cutting-edge technique in 3D computer graphics that uses neural networks to generate novel views of complex 3D scenes. Unlike traditional graphics pipelines'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='graphics pipelines that rely on explicit geometry and texture maps, NeRF models learn an implicit volumetric representation of a scene by optimizing a continuous function that maps spatial'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='that maps spatial coordinates and viewing directions to color and volume density. This function is parameterized by a multilayer perceptron (MLP), allowing the scene to be rendered with'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='to be rendered with photorealistic detail and consistent lighting effects.'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='One of the core innovations in NeRF is the use of volumetric rendering, where a ray is cast from the camera into the scene and sampled at multiple depths. At each sample point, the network predicts'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='network predicts both the color and the density, and the final pixel color is computed as a weighted integration along the ray. This rendering technique enables the capture of complex visual'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='of complex visual phenomena like soft shadows, semi-transparency, and view-dependent reflections, which are difficult to model with traditional rasterization approaches.'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='The training of NeRF involves optimizing the MLP to minimize the difference between rendered views and a set of input images taken from known camera poses. This makes NeRF a data-driven method'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='data-driven method reliant on multi-view consistency. However, this also introduces a computational bottleneck: training can take hours and rendering each frame requires hundreds of MLP evaluations.'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='of MLP evaluations. To address this, subsequent research has focused on accelerating both training and inference using hierarchical sampling, voxel grids, hash encoding, and GPU parallelism.'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='A particularly interesting crossover with machine learning comes from the application of positional encoding to enable MLPs to better represent high-frequency details. Positional encoding injects'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='encoding injects high-frequency functions of the input coordinates into the network, allowing it to overcome the spectral bias of standard MLPs that favor low-frequency outputs. This concept has'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='This concept has strong ties to similar strategies used in transformer architectures for natural language processing, showcasing an elegant convergence of ideas across domains.'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='Another significant evolution has been the adaptation of NeRF-like models for dynamic scenes, where the geometry and appearance change over time. Techniques such as D-NeRF or Neural Scene Flow Fields'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='Scene Flow Fields extend the static NeRF framework to learn temporally coherent representations, capturing motion and deformation in the scene. These advances are crucial for applications like 4D'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='like 4D reconstruction, virtual avatars, and telepresence, where both spatial and temporal fidelity are necessary.'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='In the context of 3D graphics pipelines, integrating NeRF with traditional graphics tools poses challenges, such as converting neural representations into mesh-based formats usable in real-time'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='usable in real-time engines. Hybrid approaches attempt to bridge this by distilling neural fields into polygonal meshes or texture maps using differentiable surface extraction techniques. These'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='techniques. These methods allow neural representations to benefit from hardware-accelerated rasterization, pushing toward real-time performance.'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='Lastly, the interplay between inverse rendering and generative models has opened up new frontiers in content creation. By coupling NeRFs with generative adversarial networks (GANs) or diffusion'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='(GANs) or diffusion models, researchers can synthesize plausible 3D content from minimal inputs like single images or text prompts. This fusion leverages the realism of learned priors and the'),\n",
       " Document(metadata={'source': '../data/sample_text.txt'}, page_content='priors and the physical consistency of volumetric rendering, paving the way for highly expressive and data-efficient tools in animation, gaming, and virtual reality.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 20\n",
    ")\n",
    "\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46f73366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamza\\AppData\\Local\\Temp\\ipykernel_1964\\3781062547.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09c93e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (3, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"This is a sample sentence for embeddings.\",\n",
    "    \"RAG applications are very powerful.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings using embed_documents\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "embeddings = np.array(embeddings)\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7aa452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(text_chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b0b806f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1cd01dee3d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Perform similarity search\n",
    "query = \"What is the main difference between NeRF and traditional 3D graphics pipelines?\"\n",
    "#query = \"How does NeRF use volumetric rendering to produce photorealistic images?\"\n",
    "#query = \"What techniques have been developed to speed up NeRF training and inference?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(query, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a3aa3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "In the context of 3D graphics pipelines, integrating NeRF with traditional graphics tools poses challenges, such as converting neural representations into mesh-based formats usable in real-time\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "graphics pipelines that rely on explicit geometry and texture maps, NeRF models learn an implicit volumetric representation of a scene by optimizing a continuous function that maps spatial\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "One of the core innovations in NeRF is the use of volumetric rendering, where a ray is cast from the camera into the scene and sampled at multiple depths. At each sample point, the network predicts\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "The training of NeRF involves optimizing the MLP to minimize the difference between rendered views and a set of input images taken from known camera poses. This makes NeRF a data-driven method\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5746fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an AI assistant for question-answering tasks. \n",
    "Use the retrieved context to answer the question as accurately as possible. \n",
    "If the answer is not in the context, respond with \"I don't know.\" \n",
    "Keep your answer concise and no longer than ten sentences.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "386679fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nYou are an AI assistant for question-answering tasks. \\nUse the retrieved context to answer the question as accurately as possible. \\nIf the answer is not in the context, respond with \"I don\\'t know.\" \\nKeep your answer concise and no longer than ten sentences.\\n\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "440c56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm_model = ChatGroq(model_name=\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69bd3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a2d9df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NeRF uses volumetric rendering by casting a ray from the camera into the scene and sampling many points along that ray.  \\nAt each sampled depth the network (an MLP) predicts a density (volume density) and an RGB color for that point.  \\nThese predictions are treated as a continuous radiance field that represents the sceneâ€™s geometry and appearance implicitly.  \\nThe algorithm then integrates the predicted densities and colors along the ray using the volume rendering equation, which accumulates color while attenuating light according to the density.  \\nThe integrated result for each pixel yields a photorealistic image.  \\nTraining optimizes the MLP to minimize the difference between these rendered images and a set of input photographs taken from known camera poses.  \\nThus NeRF learns a continuous volumetric representation that can generate novel views with high realism.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"How does NeRF use volumetric rendering to produce photorealistic images?\")\n",
    "#rag_chain.invoke(\"What techniques have been developed to speed up NeRF training and inference?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48c0f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.9 environment at: G:\\Data Science\\MLOPs\\LLMOPS series\\.venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 99ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 86ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstructlog\u001b[0m\u001b[2m==25.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install structlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcdcc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
